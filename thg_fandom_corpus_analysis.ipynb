{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0c20b76-6c63-4dd9-bf16-b60e583cddf4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Set Up for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0d42c4-2f63-4ca8-92f0-922445274a53",
   "metadata": {},
   "source": [
    "Tumblr Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea533108-f785-4a9e-8197-11b1aa01f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytumblr pyyaml rauth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962bc203-8e29-4a34-b7fe-38bb672b1f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb47dae-6569-4494-85f9-2417ae7f2e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytumblr\n",
    "import os\n",
    "import yaml\n",
    "import webbrowser\n",
    "from rauth import OAuth1Service\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, parse_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d71309-aee1-4fc8-a06e-da57d79c8e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78219584-d881-41e3-9bf3-1afbbabe66a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Tumblr App credentials\n",
    "consumer_key = \"<consumer_key>\"\n",
    "consumer_secret = \"<consumer_secret>\"\n",
    "\n",
    "# Token storage path\n",
    "tumblr_token_path = Path.home() / \".tumblr\"\n",
    "\n",
    "# Function to load tokens from file\n",
    "def load_tokens():\n",
    "    if tumblr_token_path.exists():\n",
    "        with open(tumblr_token_path, \"r\") as f:\n",
    "            return yaml.safe_load(f)\n",
    "    return None\n",
    "\n",
    "# Function to save tokens to file\n",
    "def save_tokens(tokens):\n",
    "    with open(tumblr_token_path, \"w\") as f:\n",
    "        yaml.dump(tokens, f)\n",
    "    print(f\"Tokens saved to {tumblr_token_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc8bfe3-d18d-440a-8a1d-3463cc1cc918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_oauth():\n",
    "    tumblr = OAuth1Service(\n",
    "        name='tumblr',\n",
    "        consumer_key=consumer_key,\n",
    "        consumer_secret=consumer_secret,\n",
    "        request_token_url='https://www.tumblr.com/oauth/request_token',\n",
    "        access_token_url='https://www.tumblr.com/oauth/access_token',\n",
    "        authorize_url='https://www.tumblr.com/oauth/authorize',\n",
    "        base_url='https://api.tumblr.com/v2/'\n",
    "    )\n",
    "    # Step 1: Get request token\n",
    "    request_token, request_token_secret = tumblr.get_request_token(params={'oauth_callback': 'http://www.example.com'})\n",
    "    authorize_url = tumblr.get_authorize_url(request_token) \n",
    "\n",
    "    print(\"Go to this URL and authorize the app:\")\n",
    "    print(authorize_url)\n",
    "\n",
    "    webbrowser.open(authorize_url)\n",
    "\n",
    "    verifier = input(\"Paste the verifier Tumblr shows you: \").strip()\n",
    "\n",
    "    session = tumblr.get_auth_session(request_token, request_token_secret,\n",
    "                                 method='POST', data={'oauth_verifier': verifier})\n",
    "     # Print the oauth_token and oauth_token_secret\n",
    "    print(\"OAuth Token:\", session.access_token)\n",
    "    print(\"OAuth Token Secret:\", session.access_token_secret)\n",
    "\n",
    "    return {\n",
    "        'consumer_key': consumer_key,\n",
    "        'consumer_secret': consumer_secret,\n",
    "        'oauth_token': session.access_token,\n",
    "        'oauth_token_secret': session.access_token_secret\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33200c9e-afbd-4e1a-8bb2-8ab926f0743e",
   "metadata": {},
   "source": [
    "__After running the snippet below, and authorizing the app, the verifier is found in the URL after 'oauth_verifier='__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f6188d-36c8-42e0-b19a-c6f3177ae19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = do_oauth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d3b1e0-8c8a-4d1a-8e4d-41cfc688b25d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Corpus Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5327ff-6f44-438b-9513-d0a453795134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in these to make it easier to use again\n",
    "consumer_key = '<consumer_key>'\n",
    "consumer_secret = '<consumer_secret>'\n",
    "oauth_token = '<oauth_token>'\n",
    "oauth_token_secret = '<oauth_secret>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4841f4-ff33-4cf8-836a-8d1e5ec1ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pytumblr.TumblrRestClient(\n",
    "    '<consumer_key>',\n",
    "    '<consumer_secret>',\n",
    "    '<oauth_token>',\n",
    "    '<oauth_secret>',\n",
    ")\n",
    "\n",
    "blog_name = '<replace with link to tumblr page>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e05441-8bdb-4fa4-9934-b59553014e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1511708-bb8e-4ed7-97fd-fb16f7fef6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b43b6d-651a-47c2-a9dd-cff0c4091a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f438dfcb-1b4a-4de7-aad8-2cc67c6b4c6c",
   "metadata": {},
   "source": [
    "## Collecting Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eef313-ed4d-431e-880d-82001cb1e155",
   "metadata": {},
   "source": [
    "### Create Character Tags and Variations (in bulk or individually)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d63ca-3d1f-4387-bfe1-e9f1d163410c",
   "metadata": {},
   "source": [
    "Character tag creation in bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef2e4ae-cbbe-4512-ae51-f0752e54ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of characters \n",
    "characters = ['alma coin', 'annie cresta', 'asterid everdeen', 'atala', 'beetee latier', 'blight', 'boggs', 'bonnie', 'bristel', \n",
    "              'brutus', 'burdock everdeen', 'buttercup', 'caesar flickerman', 'cashmere', 'cassia', 'castor', 'cato hadley', 'cecelia', 'chaff', 'cinna', \n",
    "              'claudius templesmith',\n",
    "              'clayton', 'clove', 'coriolanus snow', 'cray', 'cressida', 'darius', 'delly cartwright', 'effie trinket', 'enobaria', \n",
    "              'finnick odair', 'flavius', 'fulvia cardew', 'gale hawthorne', 'glimmer', 'gloss', 'goat man', 'greasy sae', 'haymitch abernathy',\n",
    "              'hazelle hawthorne', 'homes', 'jackson', 'johanna mason', 'katniss everdeen', 'lady', 'lavinia', 'leevy', 'lyle whitcomb', 'lyme',\n",
    "              'madge undersee', 'mags flanagan', 'martin', 'marvel', 'mayor undersee', 'messalla', 'miles', 'mitchell', 'morphlings', 'mrs mellark',\n",
    "              'mrs. mellark', 'octavia', 'otho mellark', 'mr mellark', 'mrs everdeen', 'mr everdeen', 'otto', 'peeta mellark', \n",
    "              'plutarch heavensbee', 'pollux', 'portia', 'posy hawthorne', 'primrose everdeen', 'prim everdeen', 'purnia', 'paylor', 'ripper', 'romulus thread', \n",
    "              'rory hawthorne', 'rue', 'seeder', 'seneca crane', 'thresh', 'tigris snow', 'titus', 'twill', 'venia', 'wiress', 'woof', \n",
    "              'president snow', 'president coin'\n",
    "             ]\n",
    "#function to create print common variations for every character in the list above, accounting for character without last name with an else statement\n",
    "#list for each character\n",
    "def create_tags_per_character(list):\n",
    "    for item in list: \n",
    "        if len(item.split()) > 1:\n",
    "            first_name = item.split()[0]\n",
    "            joined = item.replace(\" \", \"\")\n",
    "            print(f\"['{first_name}', 'thg {first_name}', 'the hunger games {first_name}', 'hunger games {first_name}', '{first_name} thg', '{first_name} hg', 'hg {first_name}','{first_name} hunger games', \")\n",
    "            print(f\"'{joined}', 'thg {joined}', 'the hunger games {joined}', 'hunger games {joined}', '{joined} thg', '{joined} hg', 'hg {joined}','{joined} hunger games', \")\n",
    "            print(f\"'{item}', 'thg {item}', 'the hunger games {item}', 'hunger games {item}', '{item} thg', '{item} hg', 'hg {item}','{item} hunger games'] ---- \")\n",
    "        else:\n",
    "            print(f\"['{item}', 'thg {item}', 'the hunger games {item}', 'hunger games {item}', '{item} thg', '{item} hg', 'hg {item}','{item} hunger games'] ---- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93228c1f-bc16-4a75-a4f0-30ab2a6bd31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this function to create tag variations for individual characters\n",
    "def ch_tags(name):\n",
    "    first_name = name.split()[0]\n",
    "    joined = name.replace(\" \", \"\")\n",
    "    print(f\"'{first_name}', 'thg {first_name}', 'the hunger games {first_name}', 'hunger games {first_name}', '{first_name} thg', '{first_name} hg', 'hg {first_name}','{first_name} hunger games', \")\n",
    "    print(f\"'{joined}', 'thg {joined}', 'the hunger games {joined}', 'hunger games {joined}', '{joined} thg', '{joined} hg', 'hg {joined}','{joined} hunger games', \")\n",
    "    print(f\"'{name}', 'thg {name}', 'the hunger games {name}', 'hunger games {name}', '{name} thg', '{name} hg', 'hg {name}','{name} hunger games', \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e437bb-5337-453e-81d4-1532a058503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial list of tags. adapt for your purposes if you choose not to use the functions above\n",
    "tag_katniss = ['katniss everdeen', 'thg katniss', 'the hunger games katniss']\n",
    "tag_peeta = ['peeta mellark', 'thg peeta', 'the hunger games peeta']\n",
    "tag_everlark = ['everlark', 'katniss x peeta', 'peeta x katniss'] \n",
    "tag_gale = ['gale hawthorne', 'thg gale', 'the hunger games gale', 'gale']\n",
    "tag_prim = ['primrose everdeen', 'thg prim', 'prim', 'the hunger games prim']\n",
    "tag_haymitch = ['haymitch abernathy', 'thg haymitch', 'the hunger games haymitch', 'haymitch']\n",
    "tag_effie = ['effie trinket', 'thg effie', 'the hunger games effie', 'effie']\n",
    "tag_finnick = ['finnick odair', 'thg finnick', 'the hunger games finnick', 'finnick']\n",
    "tag_beetee = ['beetee latier', 'thg beetee', 'the hunger games beetee', 'beetee']\n",
    "tag_coin = ['alma coin', 'president coin']\n",
    "tag_snow = ['coriolanus snow', 'president snow']\n",
    "tag_ph = ['plutarch heavensbee', 'thg plutarch']\n",
    "tag_rue = ['thg rue', 'the hunger games rue', 'rue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce20ee35-bb0a-4c8a-914f-8d656606fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of tags excluded for various reasons. \n",
    "#I excluded fanfiction related ones - like non-canon relationships and tags related to writing\n",
    "excluded_tags = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce40a86f-df4f-40d4-8a9a-425ce6c44069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_duplicates(df):\n",
    "    # Remove duplicates based on 'post_id' and keep the first occurrence\n",
    "    df_cleaned = df.drop_duplicates(subset='post_id', keep='first').reset_index(drop=True)\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b60917-c1e4-40d6-b8d7-0e05f5df42d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this function to collect posts adjusting the parameters as needed\n",
    "def collect_text_posts_for_tags(\n",
    "    client,\n",
    "    tags, #(list[str]): Tags to collect posts for\n",
    "    excluded_tags=None, #(list[str]): Tags to exclude posts containing them\n",
    "    max_posts_per_tag=1000, #Maximum number of posts per tag to collect.\n",
    "    total_posts_to_collect=5000, #Total maximum posts to collect among all tags.\n",
    "    posts_per_fetch=20 #Number of posts to fetch per API call (max 20 allowed by Tumblr)\n",
    "):\n",
    "    excluded_tags = excluded_tags or []\n",
    "    normalized_excluded_tags = set(et.lower() for et in excluded_tags)\n",
    "    collected_posts = []\n",
    "    collected_post_ids = set()\n",
    "\n",
    "    total_collected = 0\n",
    "\n",
    "    for tag in tags:\n",
    "        print(f\"\\nStarting collection for tag '{tag}'\")\n",
    "\n",
    "        normalized_tag = tag.lower()\n",
    "        tag_collected = 0\n",
    "\n",
    "        before = None  # Timestamp to paginate: fetch posts before this timestamp\n",
    "\n",
    "        while tag_collected < max_posts_per_tag and total_collected < total_posts_to_collect:\n",
    "            posts, oldest_ts = fetch_tagged_posts(client, tag, posts_per_fetch, before)\n",
    "\n",
    "            if not posts:\n",
    "                print(f\"No more posts returned for tag '{tag}'. Ending this tag.\")\n",
    "                break\n",
    "\n",
    "            new_posts_in_batch = 0\n",
    "\n",
    "            for post in posts:\n",
    "                # Check post type text\n",
    "                if post.get('type') != 'text':\n",
    "                    continue\n",
    "\n",
    "                # Normalize post tags to lowercase for filtering\n",
    "                post_tags = [t.lower() for t in post.get('tags', [])]\n",
    "\n",
    "                # Check tag presence and exclusion tags\n",
    "                if normalized_tag not in post_tags:\n",
    "                    continue\n",
    "                if any(ex_tag in post_tags for ex_tag in normalized_excluded_tags):\n",
    "                    continue\n",
    "\n",
    "                post_id = post.get('id')\n",
    "                if post_id in collected_post_ids:\n",
    "                    continue  # Duplicate post already collected\n",
    "\n",
    "                # Prepare row data\n",
    "                row = {\n",
    "                    'post_id': post_id,\n",
    "                    'date': post.get('date'),\n",
    "                    'type': post.get('type'),\n",
    "                    'text': post.get('body', post.get('summary', '')),\n",
    "                    'tags': ', '.join(post.get('tags', []))\n",
    "                }\n",
    "\n",
    "                collected_posts.append(row)\n",
    "                collected_post_ids.add(post_id)\n",
    "                tag_collected += 1\n",
    "                total_collected += 1\n",
    "                new_posts_in_batch += 1\n",
    "\n",
    "                print(f\"Collected {total_collected} total posts; {tag_collected} posts for tag '{tag}'\")\n",
    "\n",
    "                if tag_collected >= max_posts_per_tag or total_collected >= total_posts_to_collect:\n",
    "                    break\n",
    "\n",
    "            if new_posts_in_batch == 0:\n",
    "                print(f\"No new valid posts in this fetch for tag '{tag}'. Ending tag early to avoid endless loop.\")\n",
    "                break\n",
    "\n",
    "            before = oldest_ts  # Use oldest post timestamp for next pagination\n",
    "            time.sleep(2)  # Be gentle to API rate limits\n",
    "\n",
    "    print(\"\\nCollection complete. Deduplicating posts...\")\n",
    "\n",
    "    # Create DataFrame and remove duplicates by post_id keeping first occurrence\n",
    "    df = pd.DataFrame(collected_posts)\n",
    "    df = df.drop_duplicates(subset='post_id', keep='first').reset_index(drop=True)\n",
    "\n",
    "    print(f\"Total unique text posts collected: {len(df)}\")\n",
    "\n",
    "    # Save dataframe as CSV with tag list name in filename\n",
    "    tags_filename_part = \"_\".join([t.replace(\" \", \"_\") for t in tags])\n",
    "    csv_filename = f'tumblr_text_posts_{tags_filename_part}.csv'\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Data saved to CSV file: {csv_filename}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fccd38-576c-4124-8229-5c730b938dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run for every relevant character\n",
    "collect_text_posts_for_tags(\n",
    "    client,\n",
    "    tags, #(list[str]): Tags to collect posts for\n",
    "    excluded_tags=None, #(list[str]): Tags to exclude posts containing them\n",
    "    max_posts_per_tag=1000, #Maximum number of posts per tag to collect.\n",
    "    total_posts_to_collect=5000, #Total maximum posts to collect among all tags.\n",
    "    posts_per_fetch=20 #Number of posts to fetch per API call (max 20 allowed by Tumblr)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a95bae9-c477-4510-9610-9c9ca4137334",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tag Frequency Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34285f85-64d4-4b90-8079-3faa483da51f",
   "metadata": {},
   "source": [
    "## Preprocess for tag-frequency-analysis and text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926d0e10-025c-4933-85d4-f56660499476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataframes of collected posts\n",
    "character_df = pd.read_csv(\"<insert .csv files saved from previous step\")\n",
    "#repeat as many times as needed\n",
    "#eg: katniss_df = pd.read_csv(\"tumblr_text_posts_katniss_everdeen_thg_katniss_the_hunger_games_katniss.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c26382-f039-4877-b002-48e0928495db",
   "metadata": {},
   "source": [
    "Use the functions below as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb951d03-2594-4136-a225-ca510482e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_tags(tags):\n",
    "    tag_list = []\n",
    "    for tag in tags:\n",
    "        tag = tag.lower()\n",
    "        # Remove non-ASCII characters and emojis\n",
    "        tag = re.sub(r'[^\\x00-\\x7F]+', '', tag)  # Remove non-ASCII characters\n",
    "        tag_list.append(tag)     \n",
    "    return tag_list\n",
    "    \n",
    "def tag_counter(df):\n",
    "    # Clean the tags column to remove non-ASCII characters before splitting\n",
    "    df['tags'] = df['tags'].apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', x) if isinstance(x, str) else x)\n",
    "    \n",
    "    # Split the tags by comma and strip whitespace\n",
    "    df['tags'] = df['tags'].apply(lambda x: [tag.strip() for tag in x.split(',')] if isinstance(x, str) else x)\n",
    "    \n",
    "    # Clean tags immediately after conversion\n",
    "    df['tags'] = df['tags'].apply(lambda tags: [re.sub(r'[^\\x00-\\x7F]+', '', tag.lower()) for tag in tags])\n",
    "    \n",
    "    df_rel_tags = [tag for tags in df['tags'] for tag in tags]\n",
    "    df_taglist = norm_tags(df_rel_tags)\n",
    "    df_tag_counts = Counter(df_taglist)\n",
    "    return df_tag_counts\n",
    "\n",
    "def print_low_freq_tags(df, threshold):\n",
    "    # Get the tag counts\n",
    "    tag_counts = tag_counter(df)\n",
    "    # Filter tags with frequency less than the threshold\n",
    "    low_freq_tags = [tag for tag, count in tag_counts.items() if count < threshold]\n",
    "    # Print the low frequency tags\n",
    "    return low_freq_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9855f03-5241-42ed-a81b-1a0e1669daa8",
   "metadata": {},
   "source": [
    "Use __print_low_freq_tags(df, threshold)__ to further filter tags and add to excluded tags list.\n",
    "Repeat with every dataframe if required/desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a46c1d-242f-4338-ab0b-0b4485160549",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_freq = tag_counter(character_df)\n",
    "ch_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e17b74-82d1-468a-b0a5-bcc8c36cc583",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_low_freq_tags(character_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6209cc-e590-48a3-b082-c89db89134b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add tags to exclude/dataframe to excluded_tags, or individual lists per dataframe\n",
    "ch_excl_tags = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701ed301-24ea-4c76-a3eb-827ccd2d8654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if done individually, combine them all\n",
    "exclude_all = ch_excl_tags1 + ch_excl_tags2 + ch_excl_tags3\n",
    "masterlist = list(set(exclude_all))\n",
    "masterlist = sorted(masterlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28148eb-692c-4833-9646-11334a9a69c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy output of this cell into excluded_tags list\n",
    "print(masterlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7bd86f-2b5e-4093-a2c8-b410161e9a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat collection for final dataframe per character \n",
    "collect_text_posts_for_tags(\n",
    "    client,\n",
    "    tags, #(list[str]): Tags to collect posts for\n",
    "    excluded_tags=None, #(list[str]): Tags to exclude posts containing them\n",
    "    max_posts_per_tag=1000, #Maximum number of posts per tag to collect.\n",
    "    total_posts_to_collect=5000, #Total maximum posts to collect among all tags.\n",
    "    posts_per_fetch=20 #Number of posts to fetch per API call (max 20 allowed by Tumblr)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22788c47-68fa-42ab-bbe0-3306b05f2e10",
   "metadata": {},
   "source": [
    "## Tag Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6601ed19-91f0-43ec-a10b-c9e350fcd2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee11aff-594a-4441-9d19-4cae2b5a9382",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_tags = char_df['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1126976-27cb-4f18-9ada-67f051091243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates dictionary of tags and frequency\n",
    "all_char_tags = char_df['tags'].str.cat(sep=', ').split(', ')\n",
    "char_tags_count = Counter(all_char_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ff0da-e1a2-429e-a762-5507f111841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list tags that are unrelated (eg: character from another fictional universe, etc.)\n",
    "remove_tags = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13402643-736f-4ad9-97ab-d74fbbc67207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing unrelated posts using tags\n",
    "char_df = char_df[~char_df['tags'].str.contains('|'.join(remove_tags), case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a247dab2-9220-4cd1-a192-1f8981ab88bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess text columns\n",
    "char_df['text'] = char_df['text'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text() if pd.notnull(x) else '')\n",
    "char_df['text'] = char_df['text'].str.replace('\\n', ' ', regex=False).str.replace('\\r', ' ', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9313383-5666-47e2-a49d-6b851178b491",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_df.to_csv('char_processed_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f196854-f216-4db8-adcd-d86ee9f9fc63",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Preparing for using Riveter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a729d27-0d53-42d8-9523-9e613352b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for riveter\n",
    "!pip install -U spacy-experimental\n",
    "!pip install https://github.com/explosion/spacy-experimental/releases/download/v0.6.0/en_coreference_web_trf-3.4.0a0-py3-none-any.whl\n",
    "#egg=en_coreference_web_trf\n",
    "!spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef38d6b-769f-455c-b09e-a8adb5940636",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd #to confirm directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc7cd8b-c5a3-4e5e-90ca-723e6874677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3463f0-497d-4693-aea1-a51aa8e3cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just in case it was missed earlier\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import ast\n",
    "import glob\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbbdb7f-687a-40ed-a611-a91b61a98eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPACY & COREF IMPORTS\n",
    "import spacy\n",
    "import spacy_experimental\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp_coref = spacy.load(\"en_coreference_web_trf\")\n",
    "\n",
    "nlp_coref.replace_listeners(\"transformer\", \"coref\", [\"model.tok2vec\"])\n",
    "nlp_coref.replace_listeners(\"transformer\", \"span_resolver\", [\"model.tok2vec\"])\n",
    "\n",
    "nlp.add_pipe(\"coref\", source=nlp_coref)\n",
    "nlp.add_pipe(\"span_resolver\", source=nlp_coref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8660a3-e7ee-4e5b-9802-6de9bf7f5f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from riveter import Riveter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63518cef-eb1c-4608-894f-a0ba0e4f1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for preprocessing text\n",
    "from bs4 import BeautifulSoup\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c815ca-c518-4e9b-93bb-37f75a19ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataframes\n",
    "p2_dfs = glob.glob(\"*processed_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5396089-28ac-4bbf-81d0-21e3f4ff9f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658fecf9-e89a-457d-b05e-6ac04e1eabbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(file):\n",
    "    file_df = pd.read_csv(file)\n",
    "    return file_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7832356-2c32-4d42-9899-f38844915c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all desired dfs to create corpus for riveter analysis\n",
    "p2_char1_df = convert('char1_processed_df.csv')\n",
    "p2_char2_df = convert('char2_processed_df.csv')\n",
    "p2_char3_df = convert('char3_processed_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e10eee-09a4-4c9f-bd34-b44886f7774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = [p2_char1_df, p2_char2_df, p2_char3_df]\n",
    "p2_corpus = pd.concat(all_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ba55e-9e3f-4a65-b584-4c201d56dda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f76940-37ca-41b0-93e7-00ffe963b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_2_corpus = del_duplicates(p2_corpus) #remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5addbb91-2726-4fee-990d-729c7b20ff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_2_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1231c0d2-d755-45c7-90ca-a3b1a17d341d",
   "metadata": {},
   "source": [
    "## Preprocess of Riveter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b8eb8-57da-44fc-8604-50874af8904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = ['post_id', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85fc5e7-c4d4-448b-b270-b89feb0eb252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dicts to a list of values from the dictionary\n",
    "def dict_val_to_list(my_dict):\n",
    "  if not isinstance(my_dict, dict):\n",
    "    print(\"Error: Input is not a dictionary.\")\n",
    "    return []  # Return an empty list if the input is not a dictionary\n",
    "\n",
    "  return list(my_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23f8ffe-0a09-4d8e-87a3-8efb2ed6df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html_list(html_list):\n",
    "    cleaned = []\n",
    "    for item in html_list:\n",
    "        if not isinstance(item, str):\n",
    "            # Option 1: convert to string\n",
    "            item = str(item)\n",
    "            # Option 2: skip non-string items by uncommenting the next line\n",
    "            # continue\n",
    "        soup = BeautifulSoup(item, 'html.parser')\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        text = re.sub(r'\\xa0', '', text)\n",
    "        cleaned.append(text)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c8e397-5027-444d-ab21-30658aa79521",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = phase_2_corpus[keep_columns]\n",
    "p2_id_dict = p2['post_id'].to_dict()\n",
    "p2_ids = dict_val_to_list(p2_id_dict)\n",
    "p2_body_dict = p2['text'].to_dict()\n",
    "p2_text = dict_val_to_list(p2_body_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a9f11f-5237-4539-849e-16f5d4d7fff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_texts = parse_html_list(p2_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943564f6-dcea-4b47-b291-516472097d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this dict like an alias map - use regex so model is trained to recognize different ways of naming a character\n",
    "persona_patterns_dict={'katniss': r'^katniss$|^katniss everdeen$|^girl on fire$',\n",
    "                        'peeta': r'^peeta$|^peeta mellark$|^boy with the bread$',\n",
    "                        'haymitch': r'^haymitch$|^haymitch abernathy$',\n",
    "                        'president snow': r'^coryo$|^snow$|^president snow$|^coriolanus snow$',\n",
    "                        'president coin': r'^coin$|^alma coin$|^president coin$|^d13 president$',\n",
    "                        'primrose': r'^prim$|^primrose$|^primrose everdeen$',\n",
    "                        'gale': r'^gale$|^gale hawthorne$', \n",
    "                        'finnick': r'^finnick$|^finnick odair$',\n",
    "                        'plutarch': r'^plutarch$|^plutarch heavensbee$',\n",
    "                        'annie': r'^annie$|^annie cresta$',\n",
    "                        'johanna': r'^johanna$|^johanna mason$',\n",
    "                        'rue': r'^rue$',\n",
    "                        'effie': r'^effie$|^effie trinket$',\n",
    "                        'capitol': r'^the capitol$|^capitol$|^the rich$|^the wealthy$|^government$',\n",
    "                       }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908bc1a-ed0d-48ff-ab86-dcd5c28df8cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Use Riveter - Computational Linguistic and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c625e4e-6c7c-4ee4-9f3b-dc1b9facf713",
   "metadata": {},
   "source": [
    "Warning: Training models can take an hour or more depending on the size of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d18df-d573-496e-8bbd-9145a1eccae0",
   "metadata": {},
   "source": [
    "## Sap et al's power and agency(Linguistic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a113244-f12b-4c3e-8754-fd6832724966",
   "metadata": {},
   "outputs": [],
   "source": [
    "riveter = Riveter()\n",
    "riveter.load_sap_lexicon('agency')\n",
    "riveter.train(p2_texts,\n",
    "              p2_ids,\n",
    "              persona_patterns_dict={'katniss': r'^katniss$|^katniss everdeen$|^girl on fire$',\n",
    "                        'peeta': r'^peeta$|^peeta mellark$|^boy with the bread$',\n",
    "                        'haymitch': r'^haymitch$|^haymitch abernathy$',\n",
    "                        'president snow': r'^coryo$|^snow$|^president snow$|^coriolanus snow$',\n",
    "                        'president coin': r'^coin$|^alma coin$|^president coin$|^d13 president$',\n",
    "                        'primrose': r'^prim$|^primrose$|^primrose everdeen$',\n",
    "                        'gale': r'^gale$|^gale hawthorne$', \n",
    "                        'finnick': r'^finnick$|^finnick odair$',\n",
    "                        'plutarch': r'^plutarch$|^plutarch heavensbee$',\n",
    "                        'annie': r'^annie$|^annie cresta$',\n",
    "                        'johanna': r'^johanna$|^johanna mason$',\n",
    "                        'rue': r'^rue$',\n",
    "                        'effie': r'^effie$|^effie trinket$',\n",
    "                        'capitol': r'^the capitol$|^capitol$|^the rich$|^the wealthy$|^government$',\n",
    "                       }\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc0901e-ed4e-4203-afdd-e2d56800baf6",
   "metadata": {},
   "source": [
    "Consult demo.ipynb or myriv-test.ipynb to see options for different tools and analyses options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6777e5cf-48f0-431e-a911-41ac4dfefcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d1d4bbc-09cc-4498-9805-0473dd5d585f",
   "metadata": {},
   "source": [
    "## Rashkin et. al's Connotation Frames (Sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64712b5-a209-404d-8459-3efdf88288f4",
   "metadata": {},
   "source": [
    "#### Connotation Frames available via Rashkin: \n",
    "- __effect__: whether the event denoted by a predicate is good or bad for the entity\n",
    "- __state__: the likely mental state of an entity as the result of an event\n",
    "- __value__: whether an entity is presupposed to be valuable\n",
    "- __writer_perspective__/__reader_perspective__: the directed sentiment from the writer to an entity or the _predicted_ directed sentiment from reader to an entity\n",
    "- __agent_theme_perspective__/__theme_agent_perspective__: the directed sentiment between the agent and theme (usually reciprocal and not likely to totally contradict each other). \n",
    "\n",
    "Connotation frame polarity can be positive, negative, or neutral. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f87b6-f579-454d-8979-108681606faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "riveter = Riveter()\n",
    "riveter.load_sap_lexicon('dimension')\n",
    "riveter.train(p2_texts,\n",
    "              p2_ids,\n",
    "              persona_patterns_dict={'katniss': r'^katniss$|^katniss everdeen$|^girl on fire$',\n",
    "                        'peeta': r'^peeta$|^peeta mellark$|^boy with the bread$',\n",
    "                        'haymitch': r'^haymitch$|^haymitch abernathy$',\n",
    "                        'president snow': r'^coryo$|^snow$|^president snow$|^coriolanus snow$',\n",
    "                        'president coin': r'^coin$|^alma coin$|^president coin$|^d13 president$',\n",
    "                        'primrose': r'^prim$|^primrose$|^primrose everdeen$',\n",
    "                        'gale': r'^gale$|^gale hawthorne$', \n",
    "                        'finnick': r'^finnick$|^finnick odair$',\n",
    "                        'plutarch': r'^plutarch$|^plutarch heavensbee$',\n",
    "                        'annie': r'^annie$|^annie cresta$',\n",
    "                        'johanna': r'^johanna$|^johanna mason$',\n",
    "                        'rue': r'^rue$',\n",
    "                        'effie': r'^effie$|^effie trinket$',\n",
    "                        'capitol': r'^the capitol$|^capitol$|^the rich$|^the wealthy$|^government$',\n",
    "                       }\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042ddb98-e710-4603-8ad8-ea9ca3abcae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
